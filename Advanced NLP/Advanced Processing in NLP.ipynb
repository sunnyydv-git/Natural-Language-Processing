{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f8f02e-da3d-4bd0-a63e-aa68ddd526ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e8db118-0330-4b25-98ba-43317c5a6e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sunny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sunny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sunny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca476fd5-71f4-40d7-87d1-cc8e71de774c",
   "metadata": {},
   "source": [
    "#### Text Cleaning and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e996925-5085-4451-be3a-f4fe3d0b3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ccc98b3-8074-47ec-9743-57ca014b027f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price miss great deal\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "sample_text = \"The <b>price</b> is $500!! Don't miss out on this great deal.\"\n",
    "cleaned_text = clean_text(sample_text)\n",
    "print(cleaned_text)  # Output: price miss great deal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23db19e9-af4b-4329-a980-d57f131f4653",
   "metadata": {},
   "source": [
    "#### Subword Tokenization Using SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc979dd8-bfed-4405-b961-f7c2bcc907de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'N', 'at', 'u', 'r', 'al', '▁', 'L', 'a', 'ng', 'u', 'a', 'ge', '▁', 'P', 'r', 'o', 'ce', 's', 's', 'i', 'ng', '▁', 'i', 's', '▁a', 'm', 'a', 'z', 'i', 'ng', '!']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Train SentencePiece model\n",
    "spm.SentencePieceTrainer.train(input=\"text_corpus.txt\", model_prefix=\"spm_model\", vocab_size=41)\n",
    "\n",
    "# Load trained model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"spm_model.model\")\n",
    "\n",
    "# Tokenize text\n",
    "text = \"Natural Language Processing is amazing!\"\n",
    "tokens = sp.encode_as_pieces(text)\n",
    "print(tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c2463-e640-4874-b2be-e2951fbdedfa",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition (NER) Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5714bc38-ca1a-424c-a6cf-80c43b01776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\sunny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\sunny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\sunny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "import nltk\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"maxent_ne_chunker_tab\")\n",
    "nltk.download(\"words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91825f01-414a-4356-95c1-0ff2304ced46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'PERSON'), ('Inc.', 'ORGANIZATION'), ('Steve Jobs', 'PERSON'), ('Cupertino', 'GPE'), ('California', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "def extract_named_entities(text):\n",
    "    \"\"\"Extract named entities using NLTK\"\"\"\n",
    "    words = word_tokenize(text)  # Tokenize words\n",
    "    pos_tags = pos_tag(words)  # Assign POS tags\n",
    "    named_entities = ne_chunk(pos_tags)  # Extract named entities\n",
    "\n",
    "    entities = []\n",
    "    for chunk in named_entities:\n",
    "        if hasattr(chunk, \"label\"):\n",
    "            entities.append((\" \".join(c[0] for c in chunk), chunk.label()))  # Extract entity text and label\n",
    "    return entities\n",
    "\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\"\n",
    "entities = extract_named_entities(text)\n",
    "print(entities)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bdf3cd-8d5c-47f2-80ca-3a4742f4a484",
   "metadata": {},
   "source": [
    "#### Phrase Detection using Bigram & Trigram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d8fa5d-3a6c-449f-a90c-a925e58e8426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['natural', 'language', 'processing', 'is', 'amazing'], ['deep', 'learning', 'is', 'powerful'], ['new', 'york', 'city', 'is', 'beautiful']]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "# Sample dataset\n",
    "sentences = [\n",
    "    [\"natural\", \"language\", \"processing\", \"is\", \"amazing\"],\n",
    "    [\"deep\", \"learning\", \"is\", \"powerful\"],\n",
    "    [\"new\", \"york\", \"city\", \"is\", \"beautiful\"]\n",
    "]\n",
    "\n",
    "# Train Bigram model\n",
    "bigram = Phrases(sentences, min_count=1, threshold=1)\n",
    "bigram_phraser = Phraser(bigram)\n",
    "\n",
    "# Apply bigram model\n",
    "print([bigram_phraser[sent] for sent in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d7649-b693-40a1-b6ec-527ba33fc44b",
   "metadata": {},
   "source": [
    "####  Handling Out-of-Vocabulary (OOV) Words with FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3799471-aa5e-47d1-bf82-0debcb70768e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.4704156e-03  3.6394957e-04 -1.1292394e-03 -2.0372432e-03\n",
      " -6.1567919e-04  5.3414697e-04 -4.3283504e-05 -1.2980187e-03\n",
      " -2.3745704e-03  2.2477210e-03 -2.8684249e-04  5.7080359e-04\n",
      " -2.1035390e-04 -1.5044674e-04  2.0678286e-03 -1.5373951e-03\n",
      " -1.0674075e-03 -2.1040845e-03  5.9577945e-04  3.1080056e-04\n",
      "  2.5342440e-03 -1.9656378e-03 -3.2940309e-03  4.1215256e-04\n",
      " -4.6677390e-04 -6.6642975e-04 -2.6650240e-03 -3.2043477e-04\n",
      " -1.4796470e-03  1.0263290e-03  2.9163030e-03  1.4706829e-03\n",
      " -1.7472147e-03  3.3076527e-03 -2.0157434e-03 -2.1172846e-03\n",
      " -1.3795299e-03  2.7636072e-04  3.5848671e-03  3.2415316e-03\n",
      "  1.5890662e-03  1.2637110e-03  2.7313128e-03 -2.4599303e-03\n",
      " -2.1126440e-03 -7.4765628e-04 -2.0636574e-03 -1.1671626e-03\n",
      " -2.1256353e-03  1.6825003e-04]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    [\"deep\", \"learning\", \"is\", \"great\"],\n",
    "    [\"natural\", \"language\", \"processing\", \"rocks\"],\n",
    "    [\"neural\", \"networks\", \"are\", \"powerful\"]\n",
    "]\n",
    "\n",
    "# Train FastText model\n",
    "model = FastText(sentences, vector_size=50, window=3, min_count=1, workers=4)\n",
    "\n",
    "# Handle OOV word\n",
    "oov_word = \"deeplearning\"\n",
    "vector = model.wv[oov_word]\n",
    "print(vector)  # Prints vector representation of \"deeplearning\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ec216-c3f7-49c9-b99c-e85f73684c71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
